{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pathlib, os, time\n",
        "\n",
        "print(\"TensorFlow\", tf.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmwZKCjCUE_x",
        "outputId": "318e22bd-1187-43ca-fe02-ab2257c35dad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 2.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train, ds_test = tfds.load(\n",
        "    'stanford_dogs',  # community dataset\n",
        "    split=['train', 'test'],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=False)\n",
        "\n",
        "# CLASS_NAMES = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
        "NUM_CLASSES = 120\n",
        "\n",
        "def format_example(image, label):\n",
        "    image = tf.image.resize(image, [224, 224])\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image, label\n",
        "\n",
        "BATCH = 32\n",
        "train_ds = ds_train.map(format_example, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
        "                   .cache().shuffle(1000).batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
        "test_ds  = ds_test.map(format_example, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
        "                  .cache().batch(BATCH).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "OqHHXtsouZTD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.  Build lightweight model\n",
        "# ==========================================\n",
        "base = tf.keras.applications.MobileNetV2(\n",
        "    input_shape=(224, 224, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet')\n",
        "base.trainable = False  # fine-tune later if desired\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    base,\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "CWM0Pu6vwpp2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3.  Train quickly (5 epochs is enough for > 90 %)\n",
        "# ==========================================\n",
        "EPOCHS = 3\n",
        "hist = model.fit(train_ds, epochs=EPOCHS, validation_data=test_ds)\n",
        "test_loss, test_acc = model.evaluate(test_ds, verbose=0)\n",
        "print(f\"Float model accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqfPHL7Tw5_Z",
        "outputId": "945ac8ad-9005-494d-b1e3-eb03bdc14e08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4592 - loss: 2.4132"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " 5.  Convert to TensorFlow-Lite\n",
        "# ==========================================\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "open(\"waste_float32.tflite\", \"wb\").write(tflite_model)\n",
        "print(\"Float TFLite model size:\", len(tflite_model)/1024, \"kB\")\n",
        "\n",
        "# ==========================================\n",
        "# 6.  Post-training quantisation (full-integer)\n",
        "# ==========================================\n",
        "def representative_dataset():\n",
        "    for img_batch, _ in train_ds.take(100):  # 100 batches → ~3200 images\n",
        "        yield [img_batch]\n",
        "\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_dataset\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "\n",
        "tflite_quant = converter.convert()\n",
        "open(\"waste_int8.tflite\", \"wb\").write(tflite_quant)\n",
        "print(\"Quantised TFLite model size:\", len(tflite_quant)/1024, \"kB\")\n",
        "\n",
        "# ==========================================\n",
        "# 7.  Accuracy check of quantised model\n",
        "# ==========================================\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_quant)\n",
        "interpreter.allocate_tensors()\n",
        "input_idx  = interpreter.get_input_details()[0]['index']\n",
        "output_idx = interpreter.get_output_details()[0]['index']\n",
        "\n",
        "correct = 0\n",
        "total   = 0\n",
        "for img, lab in test_ds.unbatch().batch(1):\n",
        "    img_uint8 = tf.cast(img*255, tf.uint8)\n",
        "    interpreter.set_tensor(input_idx, img_uint8)\n",
        "    interpreter.invoke()\n",
        "    pred = interpreter.get_tensor(output_idx).argmax()\n",
        "    correct += (pred == lab.numpy()[0])\n",
        "    total += 1\n",
        "quant_acc = correct / total\n",
        "print(f\"INT8 TFLite accuracy: {quant_acc:.3f}\")\n",
        "\n",
        "# ==========================================\n",
        "# 8.  Show classification example\n",
        "# ==========================================\n",
        "def infer(image):\n",
        "    img_uint8 = tf.cast(tf.image.resize(image, [224,224])*255, tf.uint8)\n",
        "    interpreter.set_tensor(input_idx, img_uint8[None])\n",
        "    interpreter.invoke()\n",
        "    return interpreter.get_tensor(output_idx)[0]\n",
        "\n",
        "sample_img, true_lab = next(iter(test_ds.unbatch().batch(1)))\n",
        "probs = infer(sample_img[0])\n",
        "plt.imshow(sample_img[0])\n",
        "plt.title(f\"True: {CLASS_NAMES[true_lab[0]]}  Pred: {CLASS_NAMES[np.argmax(probs)]}\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sCQzROmO4VDi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}